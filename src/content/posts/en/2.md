---
title: 'The fallacies of distributed systems'
date: 2025-01-13T05:00:00Z
image: /images/posts/post-2.jpg
categories:
  - Architecture
draft: false
---

Distributed systems are now the beating heart of modern applications, enabling scalability, resilience, and high performance. However, designing distributed systems involves non-trivial challenges.

Many errors stem from the so-called fallacies of distributed systems, a list drawn up by Sun Microsystems (yes, the same ones who created Java!) that highlights erroneous assumptions often made by specialists unfamiliar with these contexts.

It's very important to have the points below clear.

The network is not reliable – Network connections can fail, suffer interruptions, or be compromised by cyber attacks. Every distributed system must be designed to face these eventualities. For example, a microservice that does not handle timeouts risks leaving requests pending, causing blocks that propagate to other connected services. Robust systems must therefore provide recovery mechanisms, such as automatic retransmissions, fault detection, and failover strategies to maintain operability.

Latency is not zero – When nodes in a distributed system communicate with each other, the time required for data transfer, known as latency, can severely affect performance. Ignoring this factor is a common mistake, especially in applications that require fast responses. A "real-time" analytics system, for example, might provide outdated data if network delays are not considered. Mitigating these problems requires both optimization of communication flows and the use of nodes close to users to reduce distances.

Bandwidth is not infinite – Even the most modern networks have capacity limits. If traffic is not optimized, it's easy to reach these limits, causing slowdowns or interruptions. Think, for example, of video streaming services: sending uncompressed content can saturate the network, degrading the user experience. The solution lies in compression techniques, data aggregation, and selective transmission of only essential information.

The network is not secure – Every node added to a distributed system represents a potential vulnerability. Without adequate protection, data in transit can be intercepted, manipulated, or stolen. Security cannot be an afterthought, but must be integrated from the beginning. For example, the absence of encryption in the transmission of sensitive information could seriously compromise user privacy. Secure distributed systems include encryption protocols, strong authentication, and careful credential management.

Topology changes constantly – In a distributed environment, nodes can join, leave the network, or change position. These dynamic changes can cause errors in overly rigid configurations. A typical example is the use of a fixed IP address to connect to a server: if the server is moved or replaced, the connection fails. Well-designed systems must instead provide dynamic discovery mechanisms, capable of adapting to an ever-evolving network.

There is no single administrator – Distributed systems often operate on multiple administrative domains, each managed by different entities. This complexity can generate inconsistencies in configuration and adopted protocols. For example, a misconfigured firewall in one domain might prevent access to crucial resources in another. Cooperation between administrators and the adoption of shared standards are fundamental to avoid conflicts and ensure smooth operation.

Transport cost is not zero – Every time data is transferred between nodes, resources are consumed: time, bandwidth, and, in cloud services, money. Ignoring these costs can lead to operational inefficiencies and unexpected expenses. An application that makes unoptimized API calls, for example, might increase data transfer costs in the cloud. It is therefore essential to design workflows that minimize unnecessary transfers and leverage local or regional caches to reduce network load.

The network is not homogeneous – Networks differ in terms of speed, stability, and connection type. An application designed for wired networks might not work as well on mobile connections, characterized by higher latency and frequent interruptions. To ensure reliable operation on a global scale, distributed systems must automatically adapt to variable network conditions, providing the best possible experience regardless of the environment.

To address the challenges arising from fallacies in distributed systems, software professionals rely on established models and design patterns that improve the resilience, security, and scalability of systems.

These patterns are fundamental for managing intrinsic complexity and preventing small issues from compromising the entire system.

#### Data Management

In a distributed system, data management is crucial for maintaining consistency and performance.

Techniques like caching allow temporarily storing the most requested data in memory, reducing the load on databases and lowering access times.

This significantly improves performance, especially in high-demand scenarios, where data is frequently read but rarely modified. Additionally, data synchronization between the various nodes of the system is essential to avoid inconsistencies: without adequate synchronization, data might be outdated or contradictory across different nodes.

Data replication is one of the solutions to ensure availability and consistency, ensuring that each node has the most up-to-date information. Without these patterns, a synchronization error could lead to serious bugs or service disruptions.

#### Modular Design

Modular design is another fundamental pattern for addressing the complexity of distributed systems.

Dividing a system into independent modules or components allows managing migrations, updates, and changes in a more gradual and safe way.

Each module is designed to function autonomously, meaning that updates or changes to one module do not necessarily affect the entire system.

This separation of concerns facilitates maintenance and scalability. When a part of the system needs to be updated or improved, for example, you can intervene on the single module without needing to stop the entire system, minimizing downtime.

Additionally, this approach reduces the risk of introducing large-scale errors, since each component is tested individually before being integrated into the main system.

#### Robust Messaging

Robust messaging is essential to ensure that communication between the various nodes of a distributed system occurs securely and reliably.

In a distributed system, communications are susceptible to delays, packet losses, or even temporary failures.

To prevent these problems from compromising the entire system, it is important to implement retry patterns, which allow automatically retrying operations that did not succeed.

For example, if a microservice failed to receive a response due to a network error, a retry mechanism will ensure that the request is repeated without causing interruptions in the system flow.

Retry patterns are designed to prevent momentary failures from causing a chain reaction that could compromise the entire process.

#### Security

Security is a crucial aspect in every distributed system.

In a distributed architecture, every node is potentially a vulnerable point that can be targeted by external attacks.

Integrating security measures directly into the system design is fundamental. This includes adopting encryption to protect data in transit, advanced authentication to verify that only authorized users and systems can access data and resources, and secure management of secrets, which refers to the protection of sensitive credentials like API keys or passwords.

A secure system ensures that, even if a node is compromised, the impact on the entire system will be contained, thanks to data protection and attack response strategies.

Understanding and addressing the fallacies of distributed systems is not just a technical aspect, but represents an approach to designing systems that are resilient, secure, and scalable.

Resilience ensures that the system can continue to function correctly even in the presence of failures or unforeseen conditions, security ensures that data and communications are protected from attacks and integrity is preserved, while scalability allows adapting to increasing workloads.

In a context where cloud and distributed architectures are now the standard, every software specialist should integrate these principles from the earliest design phases, to ensure that solutions are not only efficient, but also capable of responding to the needs of an increasingly complex and interconnected digital world.
